{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfM2uTcs2Wfc"
      },
      "outputs": [],
      "source": [
        "#HSI Table\n",
        "HSI_Database = 'HSIFROM2019_copy'\n",
        "#Start Date & End Date\n",
        "\n",
        "#Graph Title\n",
        "Graph_Title = \"With Wu As Change Year (2024-2025)\"\n",
        "#File Title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGC-rUmtA9PK"
      },
      "source": [
        "# Setup and Download Source code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ohxAuINrHYU"
      },
      "outputs": [],
      "source": [
        "# @title Setup\n",
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "from google.colab import data_table\n",
        "\n",
        "project = 'stock8word' # Project ID inserted based on the query results selected to explore\n",
        "location = 'asia-southeast1' # Location inserted based on the query results selected to explore\n",
        "client = bigquery.Client(project=project, location=location)\n",
        "data_table.enable_dataframe_formatter()\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FHy1tzSrCje",
        "outputId": "331a4384-9e82-49e6-f45f-e931625083b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Cloning into 'four-pillars'...\n",
            "remote: Enumerating objects: 2321, done.\u001b[K\n",
            "remote: Counting objects: 100% (143/143), done.\u001b[K\n",
            "remote: Compressing objects: 100% (116/116), done.\u001b[K\n",
            "remote: Total 2321 (delta 49), reused 117 (delta 26), pack-reused 2178 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2321/2321), 108.85 MiB | 4.87 MiB/s, done.\n",
            "Resolving deltas: 100% (574/574), done.\n",
            "Updating files: 100% (2205/2205), done.\n",
            "app.log  drive\tfourpillars  sample_data\n",
            "/content\n",
            "Already up to date.\n",
            "app.log  drive\tfourpillars  sample_data\n",
            "Requirement already satisfied: LunarCalendar in /usr/local/lib/python3.11/dist-packages (0.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from LunarCalendar) (2.8.2)\n",
            "Requirement already satisfied: ephem>=3.7.5.3 in /usr/local/lib/python3.11/dist-packages (from LunarCalendar) (4.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from LunarCalendar) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.6.1->LunarCalendar) (1.17.0)\n",
            "Collecting git+https://github.com/Hsins/mpl-tc-fonts.git\n",
            "  Cloning https://github.com/Hsins/mpl-tc-fonts.git to /tmp/pip-req-build-d7ehv_lx\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Hsins/mpl-tc-fonts.git /tmp/pip-req-build-d7ehv_lx\n",
            "  Resolved https://github.com/Hsins/mpl-tc-fonts.git to commit 0cc159a2dbf1eb199f0bd847eabfee75403346b6\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mpl-tc-fonts==1.0.1) (3.10.0)\n",
            "Requirement already satisfied: fontTools in /usr/local/lib/python3.11/dist-packages (from mpl-tc-fonts==1.0.1) (4.56.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mpl-tc-fonts==1.0.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mpl-tc-fonts==1.0.1) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mpl-tc-fonts==1.0.1) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mpl-tc-fonts==1.0.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mpl-tc-fonts==1.0.1) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mpl-tc-fonts==1.0.1) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mpl-tc-fonts==1.0.1) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mpl-tc-fonts==1.0.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mpl-tc-fonts==1.0.1) (1.17.0)\n",
            "Requirement already satisfied: mplfinance in /usr/local/lib/python3.11/dist-packages (0.12.10b0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mplfinance) (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from mplfinance) (2.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->mplfinance) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->mplfinance) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mplfinance) (1.17.0)\n",
            "Requirement already satisfied: mplfinance in /usr/local/lib/python3.11/dist-packages (0.12.10b0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mplfinance) (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from mplfinance) (2.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mplfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->mplfinance) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->mplfinance) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mplfinance) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!rm -rf four-pillars\n",
        "!rm -rf fourpillars\n",
        "!git clone https://github.com/gazaay/four-pillars.git\n",
        "!mv four-pillars fourpillars\n",
        "!ls\n",
        "!pwd\n",
        "!cd fourpillars && git pull\n",
        "!ls\n",
        "!pip install LunarCalendar\n",
        "# install mpl-tc-fonts\n",
        "!pip install git+https://github.com/Hsins/mpl-tc-fonts.git\n",
        "!pip install mplfinance\n",
        "!pip install --upgrade mplfinance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tisnFufrgoT"
      },
      "source": [
        "# Overall planning\n",
        "1. get stock birthday with finance.get_listing_date_timestamp\n",
        "2. get stock 8w with bazi.get_heavenly_branch_ymdh_pillars_base to base_8w\n",
        "3. Define a time range star date today minus 725 days, end date today\n",
        "4. create a blank data frame, create a \"time\" column where each element are incremented by 2hours with the time range and add future 12 months\n",
        "5. use adding_8w_pillars to add pillars in and put base_8w into the dataset\n",
        "6. reusing the timerange - get stock history data with finance.get_historical_data_UTC(hong_kong_stocks[0], start_date=(current day minus 725 days), end_date=(current day), interval='1h')\n",
        "7. merge the history data with the dataset where time is the key.\n",
        "7. get stock chengseng with create_chengseng_for_dataset\n",
        "8. feature engineering\n",
        "9. split data sets for random forest training\n",
        "10. train random forest\n",
        "11. predict with random forest model with the future 12 months"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0cuFT6yrdtX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from fourpillars.app  import finance  # Assume you have finance module with functions like get_listing_date_timestamp and get_historical_data_UTC\n",
        "from fourpillars.app  import bazi # Assume you have bazi module with functions like get_heavenly_branch_ymdh_pillars_base, adding_8w_pillars, create_chengseng_for_dataset\n",
        "from fourpillars.app import chengseng\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKUqO9O7U_s6"
      },
      "outputs": [],
      "source": [
        "import random, string\n",
        "\n",
        "TOKEN_RUNTIME = random.choice(string.ascii_letters + string.digits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5JoM89Ur2W2",
        "outputId": "d15a97bb-5e6f-4cc1-f0df-4f5f7b657f99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Listing Date not found on the webpage.\n",
            "1969-11-24 09:30:00\n"
          ]
        }
      ],
      "source": [
        "# @title Step 1: Get stock birthday\n",
        "from fourpillars.app  import finance\n",
        "\n",
        "symbol = '00700'  # Replace with your stock symbol\n",
        "symbol_code = \"0700.hk\"\n",
        "\n",
        "symbol = '^HSI'\n",
        "symbol_code = \"^HSI\"\n",
        "\n",
        "try:\n",
        "  stock_birthday_timestamp = finance.get_listing_date_timestamp(symbol)\n",
        "except Exception as ex:\n",
        "  print(f\"Error: During collect birthday {ex}\")\n",
        "  stock_birthday_timestamp = datetime(1969,11,24,9,30,0)\n",
        "\n",
        "stock_birthday_timestamp = datetime(1969,11,24,9,30,0)\n",
        "subject_birthday_timestamp = datetime(1871,6,27,9,30,0)\n",
        "subject_birthday_timestamp = datetime(1948,9,9,7,30,0)\n",
        "print(stock_birthday_timestamp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqcdfz1fAbXr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pytz\n",
        "\n",
        "def convert_timestamp_to_GMT8(df):\n",
        "    # Assuming 'timestamp_column' is your timestamp column\n",
        "    # Convert it to datetime type\n",
        "    df['time'] = pd.to_datetime(df['time'])\n",
        "\n",
        "    # Define the UTC timezone\n",
        "    utc = pytz.timezone('UTC')\n",
        "\n",
        "    # Define the GMT+8 timezone\n",
        "    gmt_plus_8 = pytz.timezone('Asia/Shanghai')  # Adjust timezone as needed\n",
        "\n",
        "    # Localize the timestamp column to UTC\n",
        "    # df['time'] = df['time'].apply(lambda x: utc.localize(x))\n",
        "\n",
        "    # Convert the timestamp to GMT+8\n",
        "    df['time'] = df['time'].dt.tz_convert(gmt_plus_8)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Define your SQL query\n",
        "# sql_query = \"\"\"\n",
        "# SELECT *\n",
        "# FROM stock8word.HSI8.HSI_2017\n",
        "# \"\"\"\n",
        "sql_query = f\"\"\"\n",
        "SELECT *\n",
        "FROM stock8word.HSI8.{HSI_Database}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Execute the query and convert results to a Pandas DataFrame\n",
        "query_job = client.query(sql_query)\n",
        "results = query_job.to_dataframe()\n",
        "\n",
        "\n",
        "results = convert_timestamp_to_GMT8(results)\n",
        "\n",
        "results\n",
        "\n",
        "results[['time','Open', 'High', 'Low', 'Close']] = results[['time','open', 'high', 'low', 'close']]\n",
        "\n",
        "results_from_GCP = results[['time','Open', 'High', 'Low', 'Close']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqAF7X8FA-zZ",
        "outputId": "0d49d6de-7fc4-4152-f4fb-4f3af28fa39e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['time', 'Open', 'High', 'Low', 'Close']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_from_GCP.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxxERKpXPzq6",
        "outputId": "2be0971b-2804-4838-ba5d-0806f6912c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Listing Date not found on the webpage.\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "symbol = '00001'  # Replace with your stock symbol\n",
        "symbol_code = \"0001.hk\"\n",
        "\n",
        "try:\n",
        "  temp_bd = finance.get_listing_date_timestamp(symbol)\n",
        "except Exception as ex:\n",
        "  print(f\"Error: During collect birthday {ex}\")\n",
        "  temp_bd = datetime(1969,11,24,9,30,0)\n",
        "\n",
        "print(temp_bd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S45eqTXsEUW"
      },
      "outputs": [],
      "source": [
        "# @title Step 2: Get stock 8w\n",
        "base_8w = bazi.get_heavenly_branch_ymdh_pillars_current(stock_birthday_timestamp.year,\n",
        "                                                            stock_birthday_timestamp.month,\n",
        "                                                            stock_birthday_timestamp.day,\n",
        "                                                            stock_birthday_timestamp.hour)\n",
        "\n",
        "base_subject_8w = bazi.get_heavenly_branch_ymdh_pillars_current(subject_birthday_timestamp.year,\n",
        "                                                            subject_birthday_timestamp.month,\n",
        "                                                            subject_birthday_timestamp.day,\n",
        "                                                            subject_birthday_timestamp.hour)\n",
        "\n",
        "\n",
        "# # @title Step 2: Get stock 8w\n",
        "# base_8w = bazi.get_heavenly_branch_ymdh_pillars_current_flip_Option_2(stock_birthday_timestamp.year,\n",
        "#                                                             stock_birthday_timestamp.month,\n",
        "#                                                             stock_birthday_timestamp.day,\n",
        "#                                                             stock_birthday_timestamp.hour)\n",
        "\n",
        "# base_subject_8w = bazi.get_heavenly_branch_ymdh_pillars_current_flip_Option_2(subject_birthday_timestamp.year,\n",
        "#                                                             subject_birthday_timestamp.month,\n",
        "#                                                             subject_birthday_timestamp.day,\n",
        "#                                                             subject_birthday_timestamp.hour)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNXN0onusGWl"
      },
      "outputs": [],
      "source": [
        "# @title Step 3: Define time range\n",
        "today = datetime.now()\n",
        "# Set the time to 9:30 AM\n",
        "today = today.replace(hour=9, minute=00, second=0, microsecond=0)\n",
        "\n",
        "start_date = today - timedelta(days=1525)\n",
        "end_date = today + timedelta(days=220)\n",
        "# @title Step 4: Create a blank data frame with time column\n",
        "time_range = pd.date_range(start=start_date, end=end_date, freq='1H').union(pd.date_range(end_date, end_date + pd.DateOffset(months=12), freq='D'))\n",
        "dataset = pd.DataFrame({'time': time_range})\n",
        "# @title Step 5: Adding 8w pillars to the dataset\n",
        "dataset = chengseng.adding_8w_pillars(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eF-O0RNKsL-m"
      },
      "outputs": [],
      "source": [
        "# @title Step 6: Get stock history data\n",
        "symbol_code = '^HSI'\n",
        "historical_data = finance.get_historical_data_UTC(symbol_code, start_date=start_date, end_date=end_date, interval='1h')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SMWhELjuf7B"
      },
      "outputs": [],
      "source": [
        "historical_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RswEbC2mB5ex"
      },
      "outputs": [],
      "source": [
        "results_cutoff = results_from_GCP.copy()\n",
        "\n",
        "# Filter out rows where 'time' is older than 200 days from today\n",
        "today = pd.Timestamp.now(tz='Asia/Shanghai')\n",
        "cutoff_date = today - pd.Timedelta(days=200)\n",
        "results_cutoff = results_cutoff[results['time'] < cutoff_date]\n",
        "\n",
        "historical_data = results_from_GCP\n",
        "\n",
        "# import pandas as pd\n",
        "\n",
        "# # Assuming you have a DataFrame called Historical_data\n",
        "# # Let's say you want to remove rows where Date > certain_datetime\n",
        "\n",
        "# certain_datetime = pd.to_datetime('2023-11-10 12:00:00+08:00')  # Replace this with your desired datetime\n",
        "\n",
        "# # Convert 'Date' column to datetime\n",
        "# historical_data['Date'] = pd.to_datetime(historical_data['Date'])\n",
        "\n",
        "# # Boolean indexing to filter rows\n",
        "# historical_data_filtered = historical_data[historical_data['Date'] <= certain_datetime]\n",
        "\n",
        "# # Historical_data_filtered now contains only the rows where Date is less than or equal to certain_datetime\n",
        "\n",
        "# historical_data = historical_data_filtered\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcxrIQBLyWkT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X15gWyzLS8vR"
      },
      "outputs": [],
      "source": [
        "print(base_8w.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IspcfofYuj1X"
      },
      "outputs": [],
      "source": [
        "# Set the timezone to 'Asia/Hong_Kong'\n",
        "dataset['time'] = dataset['time'].dt.tz_localize('Asia/Hong_Kong')\n",
        "dataset['本時'] = base_8w[\"時\"]\n",
        "dataset['本日'] = base_8w[\"日\"]\n",
        "dataset['-本時'] = base_8w[\"-時\"]\n",
        "dataset['本月'] = base_8w[\"月\"]\n",
        "dataset['本年'] = base_8w[\"年\"]\n",
        "dataset['-本月'] = base_8w[\"-月\"]\n",
        "dataset['合年']= base_8w[\"合年\"]\n",
        "dataset['合月']= base_8w[\"合月\"]\n",
        "dataset['合日']= base_8w[\"合日\"]\n",
        "dataset['合時']= base_8w[\"合時\"]\n",
        "        # \"合年\": resolveHeavenlyStem(flipped_year_stem) + resolveEarthlyBranch(flipped_year_branch),\n",
        "        # \"合月\": resolveHeavenlyStem(flipped_month_stem) + resolveEarthlyBranch(flipped_month_branch),\n",
        "        # \"合日\": resolveHeavenlyStem(flipped_day_stem) + resolveEarthlyBranch(flipped_day_branch),\n",
        "        # \"合時\": resolveHeavenlyStem(flipped_hour_stem) + resolveEarthlyBranch(flipped_hour_branch),\n",
        "# dataset['time'] = dataset['time'].dt.tz_localize('Asia/Hong_Kong')\n",
        "# dataset['subject_本時'] = base_subject_8w[\"時\"]\n",
        "# dataset['subject_本日'] = base_subject_8w[\"日\"]\n",
        "# dataset['subject_-本時'] = base_subject_8w[\"-時\"]\n",
        "# dataset['subject_本月'] = base_subject_8w[\"月\"]\n",
        "# dataset['subject_本年'] = base_subject_8w[\"年\"]\n",
        "# dataset['subject_-本月'] = base_subject_8w[\"-月\"]\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ7bGsvlC_DC"
      },
      "outputs": [],
      "source": [
        "print(historical_data[\"time\"].iloc[0])\n",
        "\n",
        "print(dataset['time'].iloc[0])\n",
        "\n",
        "historical_data.set_index('time', inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMGA6TLwDVbh"
      },
      "outputs": [],
      "source": [
        "historical_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI-ygxOgsORA"
      },
      "outputs": [],
      "source": [
        "# @title Step 7: Merge history data with the dataset\n",
        "dataset = pd.merge(dataset, historical_data, left_on='time', right_index=True, how='left')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSEh5J6hsokP"
      },
      "outputs": [],
      "source": [
        "dataset.columns.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjADQf0zv6XW"
      },
      "outputs": [],
      "source": [
        "# @title Step 7 (cont.): Get stock chengseng and merge with the dataset\n",
        "\n",
        "dataset = chengseng.create_chengseng_for_dataset(dataset)\n",
        "# dataset = pd.merge(dataset, chengseng_data, on='time', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pdBg4N005rN"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEZSbvYtQybo"
      },
      "outputs": [],
      "source": [
        "# from app.haap import add_haap_features_to_df\n",
        "\n",
        "# dataset = add_haap_features_to_df(dataset)\n",
        "\n",
        "print(f\"{dataset.columns.tolist()}\")\n",
        "dataset.to_csv(f\"drive/MyDrive/01_888/Processing/Ken_{TOKEN_RUNTIME}_before_training.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0cMPBZVwFPc"
      },
      "outputs": [],
      "source": [
        "categorical_features = dataset.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8aHco0pqBRE"
      },
      "outputs": [],
      "source": [
        "categorical_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffkv_hf5xRRB"
      },
      "outputs": [],
      "source": [
        "# Define your custom encoding mapping\n",
        "custom_encoding_mapping = {\n",
        "    \"甲子\": 1, \"乙丑\": 2, \"丙寅\": 3, \"丁卯\": 4, \"戊辰\": 5, \"己巳\": 6, \"庚午\": 7, \"辛未\": 8,\n",
        "    \"壬申\": 9, \"癸酉\": 10, \"甲戌\": 11, \"乙亥\": 12, \"丙子\": 13, \"丁丑\": 14, \"戊寅\": 15, \"己卯\": 16,\n",
        "    \"庚辰\": 17, \"辛巳\": 18, \"壬午\": 19, \"癸未\": 20, \"甲申\": 21, \"乙酉\": 22, \"丙戌\": 23, \"丁亥\": 24,\n",
        "    \"戊子\": 25, \"己丑\": 26, \"庚寅\": 27, \"辛卯\": 28, \"壬辰\": 29, \"癸巳\": 30, \"甲午\": 31, \"乙未\": 32,\n",
        "    \"丙申\": 33, \"丁酉\": 34, \"戊戌\": 35, \"己亥\": 36, \"庚子\": 37, \"辛丑\": 38, \"壬寅\": 39, \"癸卯\": 40,\n",
        "    \"甲辰\": 41, \"乙巳\": 42, \"丙午\": 43, \"丁未\": 44, \"戊申\": 45, \"己酉\": 46, \"庚戌\": 47, \"辛亥\": 48,\n",
        "    \"壬子\": 49, \"癸丑\": 50, \"甲寅\": 51, \"乙卯\": 52, \"丙辰\": 53, \"丁巳\": 54, \"戊午\": 55, \"己未\": 56,\n",
        "    \"庚申\": 57, \"辛酉\": 58, \"壬戌\": 59, \"癸亥\": 60,\n",
        "    \"甲\": 201, \"乙\": 202, \"丙\": 203, \"丁\": 204, \"戊\": 205,\n",
        "    \"己\": 206, \"庚\": 207, \"辛\": 208, \"壬\": 209, \"癸\": 210,\n",
        "    \"傷官\": -310, \"七殺\": -308, \"劫財\": -306, \"比肩\": -304,\n",
        "    \"食神\": -302, \"偏印\": 302, \"偏財\": 304, \"正財\": 306,\n",
        "    \"正官\": 308, \"正印\": 310,\n",
        "    \"長生\": 402, \"沐浴\": -402, \"冠帶\": 406, \"臨官\": 408,\n",
        "    \"帝旺\": 410, \"衰\": -404, \"病\": -406, \"死\": -410,\n",
        "    \"墓庫\": -404, \"絕\": -408, \"胎\": 400, \"養\": 400,\n",
        "    \"子\": 111, \"丑\": 112, \"寅\": 101, \"卯\": 102,\n",
        "    \"辰\": 103, \"巳\": 104, \"午\": 105, \"未\": 106,\n",
        "    \"申\": 107, \"酉\": 108, \"戌\": 109, \"亥\": 110\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "UGOgO3Q_wf1s",
        "outputId": "77cec72d-cda5-4b5e-d533-adbd284aa786"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f9bbe5b4dc3a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabel_encoders\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m )\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplotting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtseries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/api/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\" public toolkit API \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m from pandas.api import (\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mindexers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minterchange\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/api/typing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# TODO: Can't import Styler without importing jinja2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# from pandas.io.formats.style import Styler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJsonReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStataReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from pandas.io.json._json import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mread_json\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mto_json\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mujson_dumps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mujson_loads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mparse_table_schema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m )\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from pandas.io.parsers.readers import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mTextFileReader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mTextParser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mread_csv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mread_fwf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTR_NA_VALUES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m from pandas.errors import (\n\u001b[1;32m     34\u001b[0m     \u001b[0mAbstractMethodError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36minit pandas._libs.parsers\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import datetime\n",
        "\n",
        "label_encoders ={}\n",
        "columns_to_ignore = ['time', 'Open_x', 'High_x', \"Low_x\", \"Close_x\", \"Volume_x\" ,'Open_y',\n",
        " 'High_y',\n",
        " 'Low_y',\n",
        " 'Close_y',\n",
        " 'Volume_y',\n",
        " 'Open',\n",
        " 'High',\n",
        " 'Low',\n",
        " 'Close',\n",
        " 'Volume',]\n",
        "\n",
        "def process_encode_data(data_, categorical_features):\n",
        "    data = data_.copy()\n",
        "    for feature in categorical_features:\n",
        "          if feature in columns_to_ignore:\n",
        "            continue\n",
        "          try:\n",
        "            # Initialize the LabelEncoder with your custom mapping\n",
        "            label_encoder = LabelEncoder()\n",
        "            label_encoder.fit(list(custom_encoding_mapping.values()))\n",
        "            encoded_values = None\n",
        "\n",
        "            try:\n",
        "              # Try to map the values using custom_encoding_mapping\n",
        "              encoded_values = data[feature].map(custom_encoding_mapping)\n",
        "            except Exception as e:\n",
        "              print(\"An error occurred:\", e)\n",
        "            # Assuming 'data' is your DataFrame and 'feature' is the column containing the large numbers\n",
        "            formatted_values = []\n",
        "\n",
        "            # If value in feature could be int or float. We will add it there. if it is not, we will\n",
        "            # add the raw value instead.\n",
        "            for value in data[feature]:\n",
        "              if isinstance(value, (int, float)):\n",
        "                  formatted_values.append(f'{value:.0f}')\n",
        "              else:\n",
        "                  formatted_values.append(value)\n",
        "\n",
        "\n",
        "            data[feature] = formatted_values\n",
        "\n",
        "\n",
        "            # Convert the column to numeric with errors='coerce' to convert non-numeric to NaN\n",
        "            numeric_data = pd.to_numeric(data[feature], errors='coerce')\n",
        "\n",
        "            if numeric_data.isnull().any():\n",
        "              # Check if any value is not found in the custom_encoding_mapping\n",
        "              if encoded_values.isnull().any() :\n",
        "                  # Fallback to using fit_transform\n",
        "                  print(feature + \" will fall back to use fit_transform\")\n",
        "                  data[feature] = label_encoder.fit_transform(data[feature])\n",
        "              else:\n",
        "                  # Use transform with your custom mapping to encode the data\n",
        "                  data[feature] = label_encoder.transform(data[feature].map(custom_encoding_mapping))\n",
        "\n",
        "              label_encoders[feature] = label_encoder\n",
        "            else:\n",
        "              print(f\"Column {feature} doesn't need to encode\")\n",
        "              data[feature] = data[feature].astype(float)\n",
        "\n",
        "\n",
        "            print(f\"Unique values for {feature}: {data[feature].unique()}\")\n",
        "            #print(f\"decode the value {label_encoders[feature].inverse_transform(data[feature])}\")\n",
        "          except Exception as e:\n",
        "            print(\"An error occurred:\", e)\n",
        "            continue\n",
        "    return data, label_encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klaPU5adw4UD"
      },
      "outputs": [],
      "source": [
        "# @title Step 8: Feature engineering (assuming you have additional features to engineer)\n",
        "\n",
        "\n",
        "data_with_custom_features, encoder = process_encode_data(dataset, categorical_features)\n",
        "\n",
        "# Create a copy of the 'Close' column shifted 90 days into the future\n",
        "# data_with_custom_features['last_90_days_close'] = data_with_custom_features['Close'].copy().dropna().shift(-90)\n",
        "\n",
        "# Fill missing values in the 'last_90_days_close' column with the current value\n",
        "# data_with_custom_features['last_90_days_close'] = data_with_custom_features['last_90_days_close'].fillna(method='ffill')\n",
        "\n",
        "data_with_custom_features.to_csv(f\"drive/MyDrive/01_888/Processing/Feature_Engineering_Encode_with_Haap_{TOKEN_RUNTIME}_before_training.csv\",)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1y0TjyiFOw-"
      },
      "outputs": [],
      "source": [
        "# Check if there are any missing values in data_with_custom_features\n",
        "has_missing_values = data_with_custom_features.isna().any().any()\n",
        "\n",
        "# Alternatively, you can use isnull()\n",
        "# has_missing_values = data_with_custom_features.isnull().any().any()\n",
        "\n",
        "if has_missing_values:\n",
        "    print(\"data_with_custom_features contains missing values.\")\n",
        "else:\n",
        "    print(\"data_with_custom_features does not contain missing values.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxx-CDhMbUWY"
      },
      "outputs": [],
      "source": [
        "data_with_custom_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmu6PZUUx3Bo"
      },
      "outputs": [],
      "source": [
        "# @title Step 9: Split datasets for random forest training\n",
        "\n",
        "# data_with_custom_features = data_with_custom_features.dropna(subset = ['time', 'Open', 'High', \"Low\", \"Close\", \"Volume\" ,])\n",
        "\n",
        "data_with_custom_features_for_training = data_with_custom_features.dropna(subset=['time', 'Open', 'High', 'Low', 'Close',\n",
        "                                                                                  # 'Volume'\n",
        "                                                                                    ])\n",
        "\n",
        "results_cutoff = data_with_custom_features_for_training.copy()\n",
        "\n",
        "# Filter out rows where 'time' is older than 200 days from today\n",
        "today_ = pd.Timestamp.now(tz='Asia/Shanghai')\n",
        "cutoff_date = today_ - pd.Timedelta(days=200)\n",
        "# results_cutoff = results_cutoff[results_cutoff['time'] < cutoff_date]\n",
        "\n",
        "features = results_cutoff.drop(['time', 'Open', 'High', \"Low\", \"Close\",\n",
        "                                                        # \"Volume\",\n",
        "                                                        # '流時',\n",
        "                                                        # '流日',\n",
        "                                                        # '-流時',\n",
        "                                                        # '流月',\n",
        "                                                        # '流年',\n",
        "                                                        # '-流月',\n",
        "                                                        '本時',\n",
        "                                                        '本日',\n",
        "                                                        '-本時',\n",
        "                                                        '本月',\n",
        "                                                        '本年',\n",
        "                                                        '-本月'\n",
        "                                                         ], axis=1)  # Adjust based on your actual features\n",
        "# labels = dataset['label']  # Adjust based on your actual labels\n",
        "\n",
        "labels = data_with_custom_features_for_training[['Open', 'High', \"Low\", \"Close\",\n",
        "                                                #  \"Volume\"\n",
        " ]]  # Adjust based on your actual labels\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7SP0FftalAU"
      },
      "outputs": [],
      "source": [
        "data_with_custom_features.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia2gTZRqFjA3"
      },
      "outputs": [],
      "source": [
        "# Example: Check data types and NaN values\n",
        "print(\"#########XXXXXXXX#############\")\n",
        "print(X_train.dtypes)\n",
        "print(\"#########YYYYYYYY#############\")\n",
        "print(y_train.dtypes)\n",
        "\n",
        "# Check for NaN values\n",
        "print(\"#########XXXXXXXX#############\")\n",
        "print(X_train.isna().sum())\n",
        "print(\"#########YYYYYYYY#############\")\n",
        "print(y_train.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtSsW1W2YHA3"
      },
      "outputs": [],
      "source": [
        "# for subject\n",
        "# base_subject_8w\n",
        "\n",
        "# dataset['time'] = dataset['time'].dt.tz_localize('Asia/Hong_Kong')\n",
        "# dataset['subject_本時'] = base_subject_8w[\"時\"]\n",
        "# dataset['subject_本日'] = base_subject_8w[\"日\"]\n",
        "# dataset['subject_-本時'] = base_subject_8w[\"-時\"]\n",
        "# dataset['subject_本月'] = base_subject_8w[\"月\"]\n",
        "# dataset['subject_本年'] = base_subject_8w[\"年\"]\n",
        "# dataset['subject_-本月'] = base_subject_8w[\"-月\"]\n",
        "\n",
        "#Time range to predict\n",
        "# get current time  8w pillar\n",
        "# chengsengfor time range\n",
        "# happ for time range\n",
        "# create features\n",
        "# process encoding\n",
        "# encoded custom features\n",
        "# save to csv\n",
        "# fill in the gaps\n",
        "# remove na\n",
        "# training sets.\n",
        "# split sets\n",
        "# training\n",
        "# test predict\n",
        "\n",
        "\n",
        "# get current time  8w pillar\n",
        "# chengsengfor time range\n",
        "# happ for time range\n",
        "# create features\n",
        "# process encoding\n",
        "# encoded custom features\n",
        "# prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sHYtdofg3R8O"
      },
      "outputs": [],
      "source": [
        "# @title Step 10: Train random forest\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "mor_model = MultiOutputRegressor(RandomForestRegressor())\n",
        "\n",
        "mor_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxkP6HP0TUPY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "predictions = mor_model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "rmse = np.sqrt(mse)\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
        "from sklearn.metrics import r2_score\n",
        "r2 = r2_score(y_test, predictions)\n",
        "print(f'R-squared (R2 Score): {r2}')\n",
        "from sklearn.metrics import explained_variance_score\n",
        "explained_var = explained_variance_score(y_test, predictions)\n",
        "print(f'Explained Variance Score: {explained_var}')\n",
        "from sklearn.metrics import median_absolute_error\n",
        "medae = median_absolute_error(y_test, predictions)\n",
        "print(f'Median Absolute Error (MedAE): {medae}')\n",
        "\n",
        "# Assuming X_train is your training data and has the feature names\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Check feature importance\n",
        "feature_importance = mor_model.estimators_[0].feature_importances_\n",
        "\n",
        "# Combine feature names with importances and sort by importance\n",
        "sorted_features = sorted(zip(feature_names, feature_importance), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print sorted feature importance\n",
        "print('Sorted Feature Importance:')\n",
        "for i, (feature_name, importance) in enumerate(sorted_features):\n",
        "    print(f'Rank {i+1}: Feature {feature_name}, Importance: {importance}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrKSMiTEebPb"
      },
      "outputs": [],
      "source": [
        "# !pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0TH0IGJzwvC"
      },
      "outputs": [],
      "source": [
        "# Get today's date and time at 9:30 AM\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "today_930 = datetime.now().replace(hour=9, minute=30, second=0, microsecond=0)\n",
        "# Convert today_930 to 'Asia/Hong_Kong' timezone\n",
        "today_930 = pd.Timestamp(today_930).tz_localize('Asia/Hong_Kong')\n",
        "\n",
        "# Filter rows where 'time' is greater than today at 9:30 AM\n",
        "future_features = data_with_custom_features.drop(['time', 'Open', 'High', \"Low\", \"Close\",\n",
        "                                                  # \"Volume\" ,\n",
        "                                                        # '流時',\n",
        "                                                        # '流日',\n",
        "                                                        # '-流時',\n",
        "                                                        # '流月',\n",
        "                                                        # '流年',\n",
        "                                                        # '-流月',\n",
        "                                                        '本時',\n",
        "                                                        '本日',\n",
        "                                                        '-本時',\n",
        "                                                        '本月',\n",
        "                                                        '本年',\n",
        "                                                        '-本月'], axis=1)\n",
        "\n",
        "# # Add any additional feature engineering steps if needed\n",
        "\n",
        "# future_features['last_90_days_close'] = data_with_custom_features['Close'].dropna().rolling(90).mean()\n",
        "\n",
        "# # Drop rows where 'last_90_days_close' is NaN\n",
        "# future_features.dropna(subset=['last_90_days_close'], inplace=True)\n",
        "\n",
        "# Make predictions\n",
        "future_predictions = mor_model.predict(future_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMKIScWlHF67"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from datetime import datetime, timedelta\n",
        "\n",
        "# # Get today's date and time at 9:30 AM\n",
        "# today_930 = datetime.now().replace(hour=9, minute=30, second=0, microsecond=0)\n",
        "# # Convert today_930 to 'Asia/Hong_Kong' timezone\n",
        "# today_930 = pd.Timestamp(today_930).tz_localize('Asia/Hong_Kong')\n",
        "\n",
        "# # Define yesterday and tomorrow at the same time as today_930\n",
        "# yesterday_930 = today_930 - timedelta(days=1)\n",
        "# tomorrow_930 = today_930 + timedelta(days=1)\n",
        "\n",
        "# # Filter data to include only yesterday, today, and tomorrow\n",
        "# mask = (data_with_custom_features['time'] >= yesterday_930) & (data_with_custom_features['time'] <= tomorrow_930)\n",
        "# filtered_data = data_with_custom_features[mask]\n",
        "\n",
        "# # Drop unnecessary columns\n",
        "# filtered_features_s = filtered_data.drop(['time', 'Open', 'High', \"Low\", \"Close\", '本時', '本日', '-本時', '本月', '本年', '-本月',\n",
        "#                                           # 'Predicted_Open', 'Predicted_High', 'Predicted_Low', 'Predicted_Close'\n",
        "# ], axis=1)\n",
        "\n",
        "# # Make predictions\n",
        "# predictions = mor_model.predict(filtered_features_s)\n",
        "# filtered_data_c = filtered_data.copy()\n",
        "# # Assuming predictions returns a DataFrame or needs reshaping if not\n",
        "# filtered_data_c[['Predicted_Open', 'Predicted_High', 'Predicted_Low', 'Predicted_Close']] = predictions\n",
        "\n",
        "# # Print predictions\n",
        "# print(predictions)\n",
        "\n",
        "# # Report on prediction outcomes\n",
        "# for i, row in filtered_data_c.iterrows():\n",
        "#     reason = \"This prediction was made because...\"  # Placeholder for actual logic\n",
        "#     print(f\"Prediction for index {i}: {row['time']} {row['Predicted_Close']}, Reason: {reason}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYEjlIbyeT7t"
      },
      "outputs": [],
      "source": [
        "# # @title Step 11: Try to understand estimation with shap\n",
        "\n",
        "# import shap\n",
        "\n",
        "# future_features_for_shap = data_with_custom_features.copy()\n",
        "# future_features_for_shap = future_features_for_shap[(future_features_for_shap['time'] >= '2024-02-23') & (future_features_for_shap['time'] <= '2024-02-25')]\n",
        "\n",
        "# # Assuming future_features contain a column named 'date' representing dates\n",
        "# # Filter future_features for the time period between Feb 23 and Feb 25\n",
        "# filtered_future_features = future_features_for_shap.drop(['time', 'Open', 'High', \"Low\", \"Close\",\n",
        "#                                                   # \"Volume\" ,\n",
        "#                                                         # '流時',\n",
        "#                                                         # '流日',\n",
        "#                                                         # '-流時',\n",
        "#                                                         # '流月',\n",
        "#                                                         # '流年',\n",
        "#                                                         # '-流月',\n",
        "#                                                         '本時',\n",
        "#                                                         '本日',\n",
        "#                                                         '-本時',\n",
        "#                                                         '本月',\n",
        "#                                                         '本年',\n",
        "#                                                         '-本月', 'Predicted_Close', 'Predicted_High', 'Predicted_Low', 'Predicted_Open'], axis=1)\n",
        "\n",
        "# prediction = mor_model.predict(filtered_future_features)\n",
        "\n",
        "# # Initialize TreeExplainer\n",
        "# explainer = shap.TreeExplainer(mor_model)\n",
        "\n",
        "# # Calculate SHAP values for the chosen instance\n",
        "# shap_values = explainer.shap_values(filtered_future_features)\n",
        "\n",
        "# # Visualize the explanation\n",
        "# shap.initjs()\n",
        "# shap.force_plot(explainer.expected_value, shap_values, filtered_future_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwbGWMlKC0B-"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47GjADOt6MPT"
      },
      "outputs": [],
      "source": [
        "data_with_custom_features[['Predicted_Open', 'Predicted_High', 'Predicted_Low', 'Predicted_Close']] = future_predictions\n",
        "# data_with_custom_features[['Predicted_Open', 'Predicted_High', 'Predicted_Low', 'Predicted_Close', 'Predicted_Volume']] = future_predictions\n",
        "data_for_web = pd.DataFrame()\n",
        "\n",
        "data_for_web[['time', 'Predicted_Open', 'Predicted_High', 'Predicted_Low', 'Predicted_Close']]= data_with_custom_features[['time', 'Predicted_Open', 'Predicted_High', 'Predicted_Low', 'Predicted_Close']]\n",
        "data_for_web['Predicted_Volume']=0\n",
        "data_for_web.to_csv(f\"drive/MyDrive/01_888/Prediction/Prediction_{today}_version.csv\")\n",
        "# Display the predictions for the future 12 months\n",
        "print(\"Predictions for the Future 12 Months:\")\n",
        "print(data_with_custom_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1f4uC4w661w"
      },
      "outputs": [],
      "source": [
        "future_features.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF6R5JEPiuGy"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/Hsins/mpl-tc-fonts.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demtHU4xkHq1"
      },
      "outputs": [],
      "source": [
        "# .tz_localize('Asia/Hong_Kong')\n",
        "selected_columns = ['time', 'Close', 'Predicted_Close']\n",
        "selected_data = data_with_custom_features[selected_columns]\n",
        "# selected_data = selected_data.dropna(subset=['Close'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLpezOnuAYCe"
      },
      "outputs": [],
      "source": [
        "print(selected_data)\n",
        "selected_data.to_csv(f\"drive/MyDrive/01_888/Processing/Predictions_Option_1_{TOKEN_RUNTIME}_before_training.csv\",)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg_UxSvSIqTu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample DataFrame\n",
        "\n",
        "df = selected_data\n",
        "# df = pd.DataFrame(data)\n",
        "\n",
        "# Ensure time column is in datetime format\n",
        "df['time'] = pd.to_datetime(df['time'])\n",
        "\n",
        "# Filter DataFrame to only include dates from 2024 to 2025\n",
        "df_filtered = df[(df['time'] >= '2024-01-01') & (df['time'] <= '2025-12-31')]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(82, 6))\n",
        "# plt.plot(df_filtered['time'], df_filtered['Close'], label='Actual Close', marker='o')\n",
        "plt.plot(df_filtered['time'], df_filtered['Predicted_Close'], label='Predicted Close', marker='.')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Close Price')\n",
        "plt.title(f'Predicted Close Prices {Graph_Title}')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Set x-axis to show monthly ticks\n",
        "plt.gca().xaxis.set_major_locator(plt.MaxNLocator(nbins=95))  # Show every month\n",
        "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m-%d'))\n",
        "\n",
        "# Rotate x-axis labels for better readability\n",
        "plt.xticks(rotation=85)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(f\"drive/MyDrive/01_888/Prediction/prediction_Master__{Graph_Title}_{end_date}_version_1.pdf\", format=\"pdf\")\n",
        "\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI72VUjL5YvD"
      },
      "outputs": [],
      "source": [
        "# Create a reverse mapping\n",
        "reverse_encoding_mapping = {v: k for k, v in custom_encoding_mapping.items()}\n",
        "encoded_value = 63\n",
        "decoded_value = reverse_encoding_mapping.get(encoded_value, f\"Unknown Encoding: {encoded_value}\")\n",
        "print(f\"Decoded value: {decoded_value}\")\n",
        "\n",
        "# next(key for key, value in custom_encoding_mapping.items() if value == label_encoders[0].inverse_transform([26]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En9yauXnXLmC"
      },
      "outputs": [],
      "source": [
        "# Assuming data_with_custom_features is your DataFrame\n",
        "row_data = data_with_custom_features.iloc[0].to_dict()\n",
        "\n",
        "# Now, print the full row of data\n",
        "print(row_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RPmf3nqQxhn"
      },
      "outputs": [],
      "source": [
        "data_with_custom_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_OcvBl51zAu"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "today = datetime.now()\n",
        "# Set the time to 9:30 AM\n",
        "today = today.replace(hour=9, minute=00, second=0, microsecond=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eem7uKDFo0fl"
      },
      "outputs": [],
      "source": [
        "# df = data_with_custom_features.copy()\n",
        "# df['流月'] = label_encoders['流月'].inverse_transform(df['流月'])\n",
        "# df['流月']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDAyFWOQqhwq"
      },
      "outputs": [],
      "source": [
        "    # # Define start and last dates for filtering\n",
        "    # df = data_with_custom_features.copy()\n",
        "    # today = pd.Timestamp('now', tz='UTC')  # Ensures 'today' is in UTC to match your time column\n",
        "    # start_date = today - timedelta(days=15)  # Today minus 15 days\n",
        "    # last_date = start_date + timedelta(days=90)  # Start date plus 8 months (approximated with 30 days per month)\n",
        "\n",
        "    # # Filter data between start date and last date\n",
        "    # filtered_df = df[(df['time'] >= start_date) & (df['time'] <= last_date)]\n",
        "\n",
        "    # # Decode the '流月' using the label encoder for '流月'\n",
        "    # df['流月'] = label_encoders['流月'].inverse_transform(df['流月'])\n",
        "\n",
        "    # # Group by '流月' and aggregate the necessary statistics\n",
        "    # results = {}\n",
        "    # grouped = filtered_df.groupby('流月')\n",
        "\n",
        "    # for name, group in grouped:\n",
        "    #   print(f\"This is the group { name}\")\n",
        "    #   # print(group)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVprtdTWZW1p"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import json\n",
        "# from datetime import datetime, timedelta\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# def read_data():\n",
        "#     # Load your data from a CSV file or another source\n",
        "#     df = data_with_custom_features.copy()\n",
        "#     return df\n",
        "\n",
        "# def calculate_percentage_change(open_price, close_price):\n",
        "#     if open_price == 0:\n",
        "#         return 0  # Prevent division by zero\n",
        "#     return ((close_price - open_price) / open_price) * 100\n",
        "\n",
        "# def process_data(df, custom_encoding_mapping):\n",
        "#     # Ensure 'time' is handled as UTC datetime\n",
        "#     df['time'] = pd.to_datetime(df['time'], utc=True)\n",
        "#     df.sort_values('time', inplace=True)  # Sort by time\n",
        "#     # Define start and last dates for filtering\n",
        "#     today = pd.Timestamp('now', tz='UTC')  # Ensures 'today' is in UTC to match your time column\n",
        "#     start_date = today - timedelta(days=15)  # Today minus 15 days\n",
        "#     last_date = start_date + timedelta(days=90)  # Start date plus 8 months (approximated with 30 days per month)\n",
        "\n",
        "#     # Filter data between start date and last date\n",
        "#     filtered_df = df[(df['time'] >= start_date) & (df['time'] <= last_date)]\n",
        "#     # Decode the '流月'\n",
        "#     df['流月'] = label_encoders['流月'].inverse_transform(df['流月'])\n",
        "\n",
        "#     # Group by '流月' and process\n",
        "#     results = {}\n",
        "#     grouped = filtered_df.groupby('流月')\n",
        "#     next_month = []\n",
        "#     for name, group in grouped:\n",
        "#         # Aggregate to get a single value for each predictive measure\n",
        "#         aggregated_data = {\n",
        "#             'date': group['time'].iloc[0].strftime('%Y-%m-%d'),\n",
        "#             'open': group['Predicted_Open'].iloc[0],  # First value in the group\n",
        "#             'close': group['Predicted_Close'].iloc[-1],  # Last value in the group\n",
        "#             'high': group['Predicted_High'].max(),  # Maximum value in the group\n",
        "#             'low': group['Predicted_Low'].min(),  # Minimum value in the group\n",
        "#             'change': calculate_percentage_change(group['Predicted_Open'].iloc[0], group['Predicted_Close'].iloc[-1])  # Change from open to close\n",
        "#         }\n",
        "#         next_month.append(aggregated_data)\n",
        "#         # results[name] = [aggregated_data]  # Store results for this group\n",
        "\n",
        "#     return next_month\n",
        "\n",
        "# def process_weekly_data(df, custom_encoding_mapping):\n",
        "#     # Ensure 'time' is handled as UTC datetime\n",
        "#     df['time'] = pd.to_datetime(df['time'], utc=True)\n",
        "#     df.sort_values('time', inplace=True)  # Sort by time\n",
        "#     # Define start and last dates for filtering\n",
        "#     today = pd.Timestamp('now', tz='UTC')  # Ensures 'today' is in UTC to match your time column\n",
        "#     start_date = today - timedelta(days=15)  # Today minus 15 days\n",
        "#     last_date = start_date + timedelta(days=90)  # Start date plus 8 months (approximated with 30 days per month)\n",
        "\n",
        "#     # Filter data between start date and last date\n",
        "#     filtered_df = df[(df['time'] >= start_date) & (df['time'] <= last_date)]\n",
        "#     # Decode the '流月'\n",
        "#     df['流月'] = label_encoders['流月'].inverse_transform(df['流月'])\n",
        "\n",
        "#     # Group by week and process\n",
        "#     weekly_data = []\n",
        "#     grouped = filtered_df.set_index('time').resample('W').agg({\n",
        "#         'Predicted Open': 'first',\n",
        "#         'Predicted Close': 'last',\n",
        "#         'Predicted High': 'max',\n",
        "#         'Predicted Low': 'min'\n",
        "#     })\n",
        "\n",
        "#     for week_start, group in grouped.iterrows():\n",
        "#         if not group.isnull().any():  # Check for any NaN values which can occur if no data is present for the week\n",
        "#             aggregated_data = {\n",
        "#                 'date': week_start.strftime('%Y-%m-%d'),\n",
        "#                 'open': group['Predicted Open'],\n",
        "#                 'close': group['Predicted Close'],\n",
        "#                 'high': group['Predicted High'],\n",
        "#                 'low': group['Predicted Low'],\n",
        "#                 'change': f\"{calculate_percentage_change(group['Predicted Open'], group['Predicted Close']):.2f}%\"\n",
        "#             }\n",
        "#             weekly_data.append(aggregated_data)\n",
        "\n",
        "#     return weekly_data\n",
        "\n",
        "# def process_year_data(df, custom_encoding_mapping):\n",
        "#     # Ensure 'time' is handled as UTC datetime\n",
        "#     df['time'] = pd.to_datetime(df['time'], utc=True)\n",
        "#     df.sort_values('time', inplace=True)  # Sort by time\n",
        "\n",
        "#     # Define start and last dates for filtering\n",
        "#     today = pd.Timestamp('now', tz='UTC')  # Ensures 'today' is in UTC to match your time column\n",
        "#     start_date = today - timedelta(days=205)  # Today minus 15 days\n",
        "#     last_date = start_date + timedelta(days=900)  # Start date plus 8 months (approximated with 30 days per month)\n",
        "\n",
        "#     # Filter data between start date and last date\n",
        "#     filtered_df = df[(df['time'] >= start_date) & (df['time'] <= last_date)]\n",
        "#     # Decode the '流月'\n",
        "#     df['流年'] = label_encoders['流年'].inverse_transform(df['流年'])\n",
        "\n",
        "#     # Group by '流月' and process\n",
        "#     results = {}\n",
        "#     grouped = filtered_df.groupby('流年')\n",
        "#     next_month = []\n",
        "#     for name, group in grouped:\n",
        "#         # Aggregate to get a single value for each predictive measure\n",
        "#         aggregated_data = {\n",
        "#             'name': group['time'].iloc[0].strftime('%Y-%m-%d'),\n",
        "#             # 'open': group['Predicted_Open'].iloc[0],  # First value in the group\n",
        "#             'value': group['Predicted_Close'].iloc[-1]\n",
        "#         }\n",
        "#         next_month.append(aggregated_data)\n",
        "#         # results[name] = [aggregated_data]  # Store results for this group\n",
        "\n",
        "#     return next_month\n",
        "\n",
        "\n",
        "# def process_day_data(df, custom_encoding_mapping):\n",
        "#     # Ensure 'time' is handled as UTC datetime\n",
        "#     df['time'] = pd.to_datetime(df['time'], utc=True)\n",
        "#     df.sort_values('time', inplace=True)  # Sort by time\n",
        "#     # Define start and last dates for filtering\n",
        "#     today = pd.Timestamp('now', tz='UTC')  # Ensures 'today' is in UTC to match your time column\n",
        "#     start_date = today - timedelta(days=3)  # Today minus 15 days\n",
        "#     last_date = start_date + timedelta(days=3)  # Start date plus 8 months (approximated with 30 days per month)\n",
        "\n",
        "#     # Filter data between start date and last date\n",
        "#     filtered_df = df[(df['time'] >= start_date) & (df['time'] <= last_date)]\n",
        "#     # Decode the '流月'\n",
        "#     df['流日'] = label_encoders['流日'].inverse_transform(df['流日'])\n",
        "\n",
        "#     # Group by '流月' and process\n",
        "#     results = {}\n",
        "#     grouped = filtered_df.groupby('流日')\n",
        "#     next_month = []\n",
        "#     for name, group in grouped:\n",
        "#         # Aggregate to get a single value for each predictive measure\n",
        "#         aggregated_data = {\n",
        "#             'date': f\"{group['time'].iloc[0]} with { calculate_percentage_change(group['Predicted_Open'].iloc[0], group['Predicted_Close'].iloc[-1])}\",\n",
        "#             'open': group['Predicted_Open'].iloc[0],  # First value in the group\n",
        "#             'close': group['Predicted_Close'].iloc[-1],  # Last value in the group\n",
        "#             'high': group['Predicted_High'].max(),  # Maximum value in the group\n",
        "#             'low': group['Predicted_Low'].min(),  # Minimum value in the group\n",
        "#         }\n",
        "#         next_month.append(aggregated_data)\n",
        "#         # results[name] = [aggregated_data]  # Store results for this group\n",
        "\n",
        "#     return next_month\n",
        "\n",
        "\n",
        "# def output_json(data):\n",
        "#     # Convert the data to JSON format with indentation for readability\n",
        "#     json_data = json.dumps(data, indent=4, default=str)\n",
        "#     return json_data\n",
        "\n",
        "# def main():\n",
        "#     # Assume label_encoder and custom_encoding_mapping are defined here\n",
        "#     # label_encoder = LabelEncoder()  # Normally you would fit this to your data beforehand\n",
        "#     custom_encoding_mapping = {'your_key': 'your_value'}  # Populate with your actual mapping\n",
        "#     print (\"Start processing data for Month by month grouping. \")\n",
        "#     df = read_data()\n",
        "#     processed_data = process_data(df, custom_encoding_mapping)\n",
        "#     json_output = output_json(processed_data)\n",
        "#     print(json_output)\n",
        "\n",
        "#     print (\"Start processing data for Weekly grouping. \")\n",
        "#     processed_data = process_weekly_data(df, custom_encoding_mapping)\n",
        "#     json_output = output_json(processed_data)\n",
        "#     print(json_output)\n",
        "\n",
        "#     print (\"Start processing data for Yearly grouping. \")\n",
        "#     processed_data = process_year_data(df, custom_encoding_mapping)\n",
        "#     json_output = output_json(processed_data)\n",
        "#     print(json_output)\n",
        "\n",
        "#     print (\"Start processing data for Day grouping. \")\n",
        "#     processed_data = process_day_data(df, custom_encoding_mapping)\n",
        "#     json_output = output_json(processed_data)\n",
        "#     print(json_output)\n",
        "\n",
        "# main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jNqeM3z9HYD"
      },
      "outputs": [],
      "source": [
        "# @title Plot the graph\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.dates as mdates\n",
        "import mpl_tc_fonts\n",
        "import pytz\n",
        "\n",
        "mpl_tc_fonts.set_font(\"Noto Sans CJK TC\")\n",
        "\n",
        "# Assuming 'time' is a regular column (not the index)\n",
        "data_with_custom_features['time'] = pd.to_datetime(data_with_custom_features['time'])\n",
        "\n",
        "def save_plot(plt, today, symbol_code, end_date):\n",
        "    # Save the plot in PDF format\n",
        "    TARGET_COLUMN = \"Ken\" + today.strftime('%Y-%m-%d')\n",
        "    file_name_predict_with = symbol_code\n",
        "    plt.savefig(f\"drive/MyDrive/01_888/Prediction/prediction_hourly_{TARGET_COLUMN}_{end_date}_{file_name_predict_with}_version.pdf\", format=\"pdf\")\n",
        "\n",
        "\n",
        "def plot_actual_vs_predicted(data_with_custom_features, today, symbol_code, end_date):\n",
        "\n",
        "    # data_with_custom_features = data_with_custom_features_.copy().dropna(subset=['Close'], inplace=True)\n",
        "    # Assuming 'time' is the index of your DataFrame\n",
        "    plt.figure(figsize=(420, 60))\n",
        "    # TODO\n",
        "    plt.plot(data_with_custom_features['time'], data_with_custom_features['Close'], label='Actual Close', marker='o')\n",
        "    plt.plot(data_with_custom_features['time'], data_with_custom_features['Predicted_Close'], label='Predicted Close', marker='o', linestyle='dashed')\n",
        "\n",
        "    plt.title('Actual vs Predicted Close Values')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Close Values')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    save_plot(today, symbol_code, end_date)\n",
        "    # # Save the plot in PDF format\n",
        "    # TARGET_COLUMN = \"Ken\" + today.strftime('%Y-%m-%d')\n",
        "    # file_name_predict_with = symbol_code\n",
        "    # plt.savefig(f\"drive/MyDrive/01_888/Prediction/prediction_hourly_{TARGET_COLUMN}_{end_date}_{file_name_predict_with}_version.pdf\", format=\"pdf\")\n",
        "\n",
        "DATE_TIME = 'time'\n",
        "\n",
        "def decode_and_annotate(row, feature, label_encoder, custom_encoding_mapping, ax, weight='normal', color='black', offset=55):\n",
        "    try:\n",
        "\n",
        "      decoded_value = next(key for key, value in custom_encoding_mapping.items() if value == label_encoder.inverse_transform([row[feature]]))\n",
        "      ax.annotate(f\"{feature}: {decoded_value}\", (row['time'], row['Predicted_Close']),\n",
        "                textcoords=\"offset points\", xytext=(5, offset), rotation=90, ha='center',\n",
        "                weight=weight, color=color)\n",
        "    except Exception as ex:\n",
        "      print(f\"error {feature} {ex}\")\n",
        "\n",
        "\n",
        "def plot_actual_vs_predicted_vLine(input_data, today, symbol_code, end_date):\n",
        "\n",
        "    data_with_custom_features = input_data.copy()\n",
        "\n",
        "    # Assuming 'time' is a column in your DataFrame with datetime values in Asia/Shanghai timezone\n",
        "    # data_with_custom_features['time'] = pd.to_datetime(data_with_custom_features['time']).dt.tz_localize('Asia/Shanghai')\n",
        "\n",
        "\n",
        "    # Assuming 'time' is the index of your DataFrame\n",
        "    plt.figure(figsize=(520, 30))\n",
        "\n",
        "    # data_with_custom_features.dropna(subset=['Close'], inplace=True)\n",
        "    # data_with_custom_features['Close'] = data_with_custom_features['Close'].fillna(data_with_custom_features_['Close'].mean())\n",
        "\n",
        "\n",
        "    data_close = data_with_custom_features.copy()\n",
        "    data_close.dropna(subset=['Close', 'time'], inplace=True)\n",
        "    # data_with_custom_features.dropna(inplace=True)\n",
        "    data_with_custom_features['Predicted_Close'] = pd.to_numeric(data_with_custom_features['Predicted_Close'], errors='coerce')\n",
        "\n",
        "    # Assuming 'time' is a column in your DataFrame with datetime values in Asia/Shanghai timezone\n",
        "    data_with_custom_features['time'] = pd.to_datetime(data_with_custom_features['time'])\n",
        "\n",
        "\n",
        "    # Plot the actual and predicted close values\n",
        "    plt.plot(data_with_custom_features['time'], data_with_custom_features['Predicted_Close'], label='Predicted Close', marker='+', linestyle='dashed', color='red')\n",
        "    # TODO\n",
        "    plt.plot(data_close['time'], data_close['Close'], label='Actual Close', marker='o', color='blue')\n",
        "\n",
        "\n",
        "    # Set x-axis ticks and labels for every 2 hours with 'yyyy.mm.dd hh mm' format\n",
        "    date_format = mdates.DateFormatter('%Y-%m-%d %H:%M:%S%z')\n",
        "    plt.gca().xaxis.set_major_formatter(date_format)\n",
        "\n",
        "    plt.xticks(pd.date_range(start=data_with_custom_features['time'].min(),\n",
        "                            end=data_with_custom_features['time'].max(), freq='2H'), rotation=45, ha='right', minor=False)\n",
        "\n",
        "\n",
        "    print(f\"ploting time range {data_with_custom_features['time'].max()} {data_with_custom_features['time'].min()}\")\n",
        "    # # Add vertical dotted lines for every 2 hours\n",
        "    # for time_point in pd.date_range(start=data_with_custom_features['time'].min(), end=data_with_custom_features['time'].max(), freq='2H'):\n",
        "    #     plt.axvline(x=time_point, color='gray', linestyle='--', linewidth=0.5)\n",
        "\n",
        "    # Increase the number of x-axis ticks\n",
        "\n",
        "\n",
        "    # num_data_points = len(data_with_custom_features[DATE_TIME])\n",
        "    # num_ticks = min(270, num_data_points)  # Limiting ticks to the number of data points\n",
        "\n",
        "\n",
        "    # x_ticks_indices = range(0, len(data_with_custom_features[DATE_TIME]), len(data_with_custom_features[DATE_TIME]) // num_ticks)\n",
        "    # x_ticks_labels = data_with_custom_features[DATE_TIME].iloc[x_ticks_indices]\n",
        "\n",
        "    # Add actual values next to each dot\n",
        "    for index, row in data_with_custom_features.iterrows():\n",
        "        plt.annotate(f\"{row['Close']:.2f}\", (row['time'], row['Close']),\n",
        "                     textcoords=\"offset points\", xytext=(5,-55), rotation=45, ha='center')\n",
        "        plt.annotate(f\"{row['Predicted_Close']:.2f}\", (row['time'], row['Predicted_Close']),\n",
        "                     textcoords=\"offset points\", xytext=(5,15),rotation=45, weight='bold', ha='center')\n",
        "        # decoded_value_流月 = label_encoder.inverse_transform([row['流月']])\n",
        "        # decoded_value_流月 = next(key for key, value in custom_encoding_mapping.items() if value == row['流月'])\n",
        "        # decoded_value_流月 = next(key for key, value in custom_encoding_mapping.items() if value == label_encoders[0].inverse_transform([row['流月']]))\n",
        "        # plt.annotate(f\"月:{decoded_value_流月}\", (row['time'], row['Predicted_Close']),\n",
        "        #              textcoords=\"offset points\", xytext=(5,55),rotation=90, ha='center')\n",
        "        # Assuming '流月' is the feature you want to decode and annotate\n",
        "        decode_and_annotate(row, '-流月', label_encoders['-流月'], custom_encoding_mapping, offset = 305, ax=plt.gca())\n",
        "        decode_and_annotate(row, '流年', label_encoders['流年'], custom_encoding_mapping, offset = 255, ax=plt.gca())\n",
        "        decode_and_annotate(row, '流月', label_encoders['流月'], custom_encoding_mapping, offset = 205, ax=plt.gca(), weight='bold', color='red')\n",
        "        decode_and_annotate(row, '-流時', label_encoders['-流時'], custom_encoding_mapping, offset = 155, ax=plt.gca())\n",
        "        decode_and_annotate(row, '流日', label_encoders['流日'], custom_encoding_mapping, offset = 105, ax=plt.gca(),  color='blue')\n",
        "        decode_and_annotate(row, '流時', label_encoders['流時'], custom_encoding_mapping, offset = 55, ax=plt.gca(),  color = 'blue')\n",
        "# '流時',\n",
        "# '流日',\n",
        "# '-流時',\n",
        "# '流月',\n",
        "# '流年',\n",
        "# '-流月',\n",
        "    # Set labels and title\n",
        "    plt.title('Actual vs Predicted Close Values')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Close Values')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Adjust y-axis limits to increase spacing\n",
        "    plt.ylim(bottom=min(data_with_custom_features[['Close', 'Predicted_Close']].min()) - 1000,\n",
        "             top=max(data_with_custom_features[['Close', 'Predicted_Close']].max()) + 1200)\n",
        "\n",
        "    # Adjust spacing around the plot\n",
        "    #plt.subplots_adjust(top=1.1, bottom=1)\n",
        "\n",
        "    # Save the plot in PDF format\n",
        "    TARGET_COLUMN = \"Ken\" + today.strftime('%Y-%m-%d%hh%mm%ss')\n",
        "    file_name_predict_with = symbol_code\n",
        "    #mbol_code\n",
        "    plt.plot()\n",
        "\n",
        "    plt.savefig(f\"drive/MyDrive/01_888/Prediction/prediction_hourly_{TARGET_COLUMN}_{end_date}_{file_name_predict_with}_version_1.pdf\", format=\"pdf\")\n",
        "\n",
        "    plt.show()\n",
        "    # Show the plot\n",
        "    plt.close()\n",
        "    #save_plot(plt.gca(), today, symbol_code, end_date)\n",
        "\n",
        "\n",
        "\n",
        "df_plot = data_with_custom_features.copy()\n",
        "# Calculate the date range\n",
        "start_date = today - datetime.timedelta(days=2)\n",
        "end_date = today + datetime.timedelta(days=90)\n",
        "\n",
        "\n",
        "start_date = pd.to_datetime(start_date).tz_localize('Asia/Hong_Kong')\n",
        "end_date = pd.to_datetime(end_date).tz_localize('Asia/Hong_Kong')\n",
        "# Filter the DataFrame based on the date range\n",
        "df_plot = df_plot[(df_plot['time']>= start_date) & (df_plot['time'] <= end_date)]\n",
        "\n",
        "\n",
        "plot_actual_vs_predicted_vLine(df_plot, today, symbol_code, end_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6eV1s9hjXbU"
      },
      "outputs": [],
      "source": [
        "# @title Plot the graph\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.dates as mdates\n",
        "import mpl_tc_fonts\n",
        "import pytz\n",
        "\n",
        "mpl_tc_fonts.set_font(\"Noto Sans CJK TC\")\n",
        "\n",
        "# Assuming 'time' is a regular column (not the index)\n",
        "data_with_custom_features['time'] = pd.to_datetime(data_with_custom_features['time'])\n",
        "\n",
        "def save_plot(plt, today, symbol_code, end_date):\n",
        "    # Save the plot in PDF format\n",
        "    TARGET_COLUMN = \"Ken\" + today.strftime('%Y-%m-%d')\n",
        "    file_name_predict_with = symbol_code\n",
        "    plt.savefig(f\"drive/MyDrive/01_888/Prediction/prediction_hourly_{TARGET_COLUMN}_{end_date}_{file_name_predict_with}_version.pdf\", format=\"pdf\")\n",
        "\n",
        "\n",
        "def plot_actual_vs_predicted(data_with_custom_features, today, symbol_code, end_date):\n",
        "\n",
        "    # data_with_custom_features = data_with_custom_features_.copy().dropna(subset=['Close'], inplace=True)\n",
        "    # Assuming 'time' is the index of your DataFrame\n",
        "    plt.figure(figsize=(420, 60))\n",
        "    # plt.plot(data_with_custom_features['time'], data_with_custom_features['Close'], label='Actual Close', marker='o')\n",
        "    plt.plot(data_with_custom_features['time'], data_with_custom_features['Predicted_Close'], label='Predicted Close', marker='o', linestyle='dashed')\n",
        "\n",
        "    plt.title('Actual vs Predicted Close Values')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Close Values')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    save_plot(today, symbol_code, end_date)\n",
        "    # # Save the plot in PDF format\n",
        "    # TARGET_COLUMN = \"Ken\" + today.strftime('%Y-%m-%d')\n",
        "    # file_name_predict_with = symbol_code\n",
        "    # plt.savefig(f\"drive/MyDrive/01_888/Prediction/prediction_hourly_{TARGET_COLUMN}_{end_date}_{file_name_predict_with}_version.pdf\", format=\"pdf\")\n",
        "\n",
        "DATE_TIME = 'time'\n",
        "\n",
        "def decode_and_annotate(row, feature, label_encoder, custom_encoding_mapping, ax, weight='normal', color='black', offset=55):\n",
        "    try:\n",
        "\n",
        "      decoded_value = next(key for key, value in custom_encoding_mapping.items() if value == label_encoder.inverse_transform([row[feature]]))\n",
        "      ax.annotate(f\"{feature}: {decoded_value}\", (row['time'], row['Predicted_Close']),\n",
        "                textcoords=\"offset points\", xytext=(5, offset), rotation=90, ha='center',\n",
        "                weight=weight, color=color)\n",
        "    except Exception as ex:\n",
        "      print(f\"error {feature} {ex}\")\n",
        "\n",
        "\n",
        "def plot_actual_vs_predicted_vLine(input_data, today, symbol_code, end_date):\n",
        "\n",
        "    data_with_custom_features = input_data.copy()\n",
        "\n",
        "    # Assuming 'time' is a column in your DataFrame with datetime values in Asia/Shanghai timezone\n",
        "    # data_with_custom_features['time'] = pd.to_datetime(data_with_custom_features['time']).dt.tz_localize('Asia/Shanghai')\n",
        "\n",
        "\n",
        "    # Assuming 'time' is the index of your DataFrame\n",
        "    plt.figure(figsize=(320, 20))\n",
        "\n",
        "    # data_with_custom_features.dropna(subset=['Close'], inplace=True)\n",
        "    # data_with_custom_features['Close'] = data_with_custom_features['Close'].fillna(data_with_custom_features_['Close'].mean())\n",
        "    data_close = data_with_custom_features.copy()\n",
        "    # data_with_custom_features.dropna(inplace=True)\n",
        "    data_close.dropna(subset=['Close', 'time'], inplace=True)\n",
        "    data_with_custom_features['Predicted_Close'] = pd.to_numeric(data_with_custom_features['Predicted_Close'], errors='coerce')\n",
        "\n",
        "    # Assuming 'time' is a column in your DataFrame with datetime values in Asia/Shanghai timezone\n",
        "    data_with_custom_features['time'] = pd.to_datetime(data_with_custom_features['time'])\n",
        "\n",
        "\n",
        "    # Plot the actual and predicted close values\n",
        "    plt.plot(data_with_custom_features['time'], data_with_custom_features['Predicted_Close'], label='Predicted Close', marker='+', linestyle='dashed', color='red')\n",
        "    plt.plot(data_close['time'], data_close['Close'], label='Actual Close', marker='o', color='blue')\n",
        "\n",
        "\n",
        "    # Set x-axis ticks and labels for every 2 hours with 'yyyy.mm.dd hh mm' format\n",
        "    date_format = mdates.DateFormatter('%Y-%m-%d %H:%M:%S%z')\n",
        "    plt.gca().xaxis.set_major_formatter(date_format)\n",
        "\n",
        "    plt.xticks(pd.date_range(start=data_with_custom_features['time'].min(),\n",
        "                            end=data_with_custom_features['time'].max(), freq='2H'), rotation=45, ha='right', minor=False)\n",
        "\n",
        "\n",
        "    print(f\"ploting time range {data_with_custom_features['time'].max()} {data_with_custom_features['time'].min()}\")\n",
        "    # # Add vertical dotted lines for every 2 hours\n",
        "    # for time_point in pd.date_range(start=data_with_custom_features['time'].min(), end=data_with_custom_features['time'].max(), freq='2H'):\n",
        "    #     plt.axvline(x=time_point, color='gray', linestyle='--', linewidth=0.5)\n",
        "\n",
        "    # Increase the number of x-axis ticks\n",
        "\n",
        "\n",
        "    # num_data_points = len(data_with_custom_features[DATE_TIME])\n",
        "    # num_ticks = min(270, num_data_points)  # Limiting ticks to the number of data points\n",
        "\n",
        "\n",
        "    # x_ticks_indices = range(0, len(data_with_custom_features[DATE_TIME]), len(data_with_custom_features[DATE_TIME]) // num_ticks)\n",
        "    # x_ticks_labels = data_with_custom_features[DATE_TIME].iloc[x_ticks_indices]\n",
        "\n",
        "    # Add actual values next to each dot\n",
        "    for index, row in data_with_custom_features.iterrows():\n",
        "        plt.annotate(f\"{row['Close']:.2f}\", (row['time'], row['Close']),\n",
        "                     textcoords=\"offset points\", xytext=(5,-55), rotation=45, ha='center')\n",
        "        plt.annotate(f\"{row['Predicted_Close']:.2f}\", (row['time'], row['Predicted_Close']),\n",
        "                     textcoords=\"offset points\", xytext=(5,15),rotation=45, weight='bold', ha='center')\n",
        "        # decoded_value_流月 = label_encoder.inverse_transform([row['流月']])\n",
        "        # decoded_value_流月 = next(key for key, value in custom_encoding_mapping.items() if value == row['流月'])\n",
        "        # decoded_value_流月 = next(key for key, value in custom_encoding_mapping.items() if value == label_encoders[0].inverse_transform([row['流月']]))\n",
        "        # plt.annotate(f\"月:{decoded_value_流月}\", (row['time'], row['Predicted_Close']),\n",
        "        #              textcoords=\"offset points\", xytext=(5,55),rotation=90, ha='center')\n",
        "        # Assuming '流月' is the feature you want to decode and annotate\n",
        "        decode_and_annotate(row, '-流月', label_encoders['-流月'], custom_encoding_mapping, offset = 305, ax=plt.gca())\n",
        "        decode_and_annotate(row, '流年', label_encoders['流年'], custom_encoding_mapping, offset = 255, ax=plt.gca())\n",
        "        decode_and_annotate(row, '流月', label_encoders['流月'], custom_encoding_mapping, offset = 205, ax=plt.gca(), weight='bold', color='red')\n",
        "        decode_and_annotate(row, '-流時', label_encoders['-流時'], custom_encoding_mapping, offset = 155, ax=plt.gca())\n",
        "        decode_and_annotate(row, '流日', label_encoders['流日'], custom_encoding_mapping, offset = 105, ax=plt.gca(),  color='blue')\n",
        "        decode_and_annotate(row, '流時', label_encoders['流時'], custom_encoding_mapping, offset = 55, ax=plt.gca(),  color = 'blue')\n",
        "# '流時',\n",
        "# '流日',\n",
        "# '-流時',\n",
        "# '流月',\n",
        "# '流年',\n",
        "# '-流月',\n",
        "    # Set labels and title\n",
        "    plt.title('Actual vs Predicted Close Values')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Close Values')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Adjust y-axis limits to increase spacing\n",
        "    plt.ylim(bottom=min(data_with_custom_features[['Close', 'Predicted_Close']].min()) - 800,\n",
        "             top=max(data_with_custom_features[['Close', 'Predicted_Close']].max()) + 600)\n",
        "\n",
        "    # Adjust spacing around the plot\n",
        "    #plt.subplots_adjust(top=1.1, bottom=1)\n",
        "\n",
        "    # Save the plot in PDF format\n",
        "    TARGET_COLUMN = \"Ken\" + today.strftime('%Y-%m-%d%hh%mm%ss')\n",
        "    file_name_predict_with = symbol_code\n",
        "    #mbol_code\n",
        "    plt.plot()\n",
        "\n",
        "    plt.savefig(f\"drive/MyDrive/01_888/Prediction/prediction_hourly_{TARGET_COLUMN}_{end_date}_{file_name_predict_with}_version_2.pdf\", format=\"pdf\")\n",
        "\n",
        "    plt.show()\n",
        "    # Show the plot\n",
        "    plt.close()\n",
        "    #save_plot(plt.gca(), today, symbol_code, end_date)\n",
        "\n",
        "df_plot = data_with_custom_features.copy()\n",
        "# Calculate the date range\n",
        "start_date = today - datetime.timedelta(days=3)\n",
        "end_date = today + datetime.timedelta(days=13)\n",
        "\n",
        "\n",
        "start_date = pd.to_datetime(start_date).tz_localize('Asia/Hong_Kong')\n",
        "end_date = pd.to_datetime(end_date).tz_localize('Asia/Hong_Kong')\n",
        "# Filter the DataFrame based on the date range\n",
        "df_plot = df_plot[(df_plot['time']>= start_date) & (df_plot['time'] <= end_date)]\n",
        "\n",
        "\n",
        "plot_actual_vs_predicted_vLine(df_plot, today, symbol_code, end_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJbOo8qmIraH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure 'time' is the DataFrame index or a column. If it's a column, adjust as needed.\n",
        "plt.figure(figsize=(41, 5))  # Set the figure size for better readability\n",
        "data_with_custom_features_= data_with_custom_features.copy()\n",
        "# data_with_custom_features_.dropna(subset=['Close'], inplace=True)\n",
        "\n",
        "# Ensure the 'time' column is in datetime format for proper plotting on the X-axis\n",
        "data_with_custom_features_['time'] = pd.to_datetime(data_with_custom_features_['time'], errors='coerce')\n",
        "# data_with_custom_features['Close'] = data_with_custom_features['Close'].fillna(data_with_custom_features_['Close'].mean())\n",
        "data_close = data_with_custom_features_.copy()\n",
        "data_close.dropna(subset=['Close', 'time'], inplace=True)\n",
        "# data_with_custom_features_['Close'] = data_with_custom_features_['Close'].fillna(0)  # or .fillna(data_with_custom_features_['Close'].mean())\n",
        "# data_with_custom_features_['Predicted_Close'] = data_with_custom_features_['Predicted_Close'].fillna(0)\n",
        "\n",
        "\n",
        "# Plotting the 'Close' and 'Predicted_Close' values, handling NaN values gracefully\n",
        "plt.plot(data_close['time'], data_close['Close'], label='Actual Close', color='blue', marker='o', linestyle='-')\n",
        "plt.plot(data_with_custom_features_['time'], data_with_custom_features_['Predicted_Close'], label='Predicted Close', color='red', linestyle='--')\n",
        "\n",
        "\n",
        "# # Convert Close and Predicted_Close to numeric types, coercing errors will turn invalid parsing to NaN\n",
        "# data_with_custom_features_['Close'] = pd.to_numeric(data_with_custom_features_['Close'], errors='coerce')\n",
        "# # data_with_custom_features_['Predicted_Close'] = pd.to_numeric(data_with_custom_features_['Predicted_Close'], errors='coerce')\n",
        "\n",
        "# # Plotting the 'Close' values\n",
        "# # plt.plot(data_with_custom_features_['time'], data_with_custom_features_['Close'], label='Actual Close', color='blue', marker='o', linestyle='-')\n",
        "# # Plotting the 'Close' values\n",
        "# # It will automatically ignore NaN values in 'Close'\n",
        "# plt.plot(data_with_custom_features_['time'], data_with_custom_features_['Close'], label='Actual Close', color='blue', marker='o', linestyle='-')\n",
        "\n",
        "# # Plotting the 'Predicted_Close' values\n",
        "# plt.plot(data_with_custom_features_['time'], data_with_custom_features_['Predicted_Close'], label='Predicted Close', color='red', marker='', linestyle='--')\n",
        "\n",
        "\n",
        "# Set x-axis ticks and labels for every 2 hours with 'yyyy.mm.dd hh mm' format\n",
        "date_format = mdates.DateFormatter('%Y-%m-%d %H:%M:%S%z')\n",
        "plt.gca().xaxis.set_major_formatter(date_format)\n",
        "\n",
        "plt.xticks(pd.date_range(start=data_with_custom_features['time'].min(),\n",
        "                        end=data_with_custom_features['time'].max(), freq='1M'), rotation=45, ha='right', minor=False)\n",
        "\n",
        "# # Add vertical dotted lines for every 2 hours\n",
        "# for time_point in pd.date_range(start=data_with_custom_features['time'].min(), end=data_with_custom_features['time'].max(), freq='2H'):\n",
        "#     plt.axvline(x=time_point, color='gray', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Increase the number of x-axis ticks\n",
        "\n",
        "\n",
        "# num_data_points = len(data_with_custom_features[DATE_TIME])\n",
        "# num_ticks = min(270, num_data_points)  # Limiting ticks to the number of data points\n",
        "\n",
        "\n",
        "# x_ticks_indices = range(0, len(data_with_custom_features[DATE_TIME]), len(data_with_custom_features[DATE_TIME]) // num_ticks)\n",
        "# x_ticks_labels = data_with_custom_features[DATE_TIME].iloc[x_ticks_indices]\n",
        "\n",
        "# Add actual values next to each dot\n",
        "# for index, row in data_with_custom_features.iterrows():\n",
        "#     plt.annotate(f\"{row['Close']:.2f}\", (row['time'], row['Close']),\n",
        "#                   textcoords=\"offset points\", xytext=(5,-55), rotation=45, ha='center')\n",
        "#     plt.annotate(f\"{row['Predicted_Close']:.2f}\", (row['time'], row['Predicted_Close']),\n",
        "#                   textcoords=\"offset points\", xytext=(5,15),rotation=45, weight='bold', ha='center')\n",
        "#     # decoded_value_流月 = label_encoder.inverse_transform([row['流月']])\n",
        "#     # decoded_value_流月 = next(key for key, value in custom_encoding_mapping.items() if value == row['流月'])\n",
        "#     # decoded_value_流月 = next(key for key, value in custom_encoding_mapping.items() if value == label_encoders[0].inverse_transform([row['流月']]))\n",
        "#     # plt.annotate(f\"月:{decoded_value_流月}\", (row['time'], row['Predicted_Close']),\n",
        "#     #              textcoords=\"offset points\", xytext=(5,55),rotation=90, ha='center')\n",
        "#     # Assuming '流月' is the feature you want to decode and annotate\n",
        "#     decode_and_annotate(row, '-流月', label_encoders['-流月'], custom_encoding_mapping, offset = 305, ax=plt.gca())\n",
        "#     decode_and_annotate(row, '流年', label_encoders['流年'], custom_encoding_mapping, offset = 255, ax=plt.gca())\n",
        "#     decode_and_annotate(row, '流月', label_encoders['流月'], custom_encoding_mapping, offset = 205, ax=plt.gca(), weight='bold', color='red')\n",
        "#     decode_and_annotate(row, '-流時', label_encoders['-流時'], custom_encoding_mapping, offset = 155, ax=plt.gca())\n",
        "#     decode_and_annotate(row, '流日', label_encoders['流日'], custom_encoding_mapping, offset = 105, ax=plt.gca(),  color='blue')\n",
        "#     decode_and_annotate(row, '流時', label_encoders['流時'], custom_encoding_mapping, offset = 55, ax=plt.gca(),  color = 'blue')\n",
        "# '流時',\n",
        "# '流日',\n",
        "# '-流時',\n",
        "# '流月',\n",
        "# '流年',\n",
        "# '-流月',\n",
        "# Set labels and title\n",
        "plt.title('Full Year Chat')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Close Values')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Adjust y-axis limits to increase spacing\n",
        "plt.ylim(bottom=min(data_with_custom_features[['Close', 'Predicted_Close']].min()) - 1000,\n",
        "          top=max(data_with_custom_features[['Close', 'Predicted_Close']].max()) + 1200)\n",
        "\n",
        "# Adjust spacing around the plot\n",
        "#plt.subplots_adjust(top=1.1, bottom=1)\n",
        "\n",
        "# Save the plot in PDF format\n",
        "TARGET_COLUMN = \"Ken\" + today.strftime('%Y-%m-%d%hh%mm%ss')\n",
        "file_name_predict_with = symbol_code\n",
        "#mbol_code\n",
        "plt.plot()\n",
        "\n",
        "plt.savefig(f\"drive/MyDrive/01_888/Prediction/prediction_year_{TARGET_COLUMN}_{end_date}_{file_name_predict_with}_version.pdf\", format=\"pdf\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "# Show the plot\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TvWkT3Fdw1-"
      },
      "outputs": [],
      "source": [
        "decripted_data = data_with_custom_features.copy()\n",
        "for index, row in data_with_custom_features.iterrows():\n",
        "  for feature in categorical_features:\n",
        "        if feature in columns_to_ignore:\n",
        "          continue\n",
        "        if row[feature] == 0:\n",
        "          continue\n",
        "        if row[feature] == 1:\n",
        "          continue\n",
        "        # print(f\"{feature} - {row[feature]}\")\n",
        "        label_decripted = label_encoders[feature].inverse_transform([row[feature]])\n",
        "        feature_decripted = next(key for key, value in custom_encoding_mapping.items() if value == label_decripted )\n",
        "        # feature_decripted = reverse_encoding_mapping.get(label_decripted, f\"Unknown Encoding: {encoded_value}\")\n",
        "        decripted_data.at[index, feature] =  feature_decripted\n",
        "        # print(f\"{feature} - {feature_decripted}\")\n",
        "decripted_data.to_csv(f\"drive/MyDrive/01_888/Prediction/feature_with_reverse_encode_{today}_version.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDjI2kRI5PGU"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "# 1. Get birthday\n",
        "trum_birthday = datetime(1964,6,14,22,45,0) #hktime\n",
        "biden_birthday = datetime(1942,11,20,20,30,0) #hktime\n",
        "\n",
        "# 2. Get 8w\n",
        "trum_birthday_8w = bazi.get_heavenly_branch_ymdh_pillars_base(trum_birthday.year,\n",
        "                                                            trum_birthday.month,\n",
        "                                                            trum_birthday.day,\n",
        "                                                            trum_birthday.hour)\n",
        "biden_birthday_8w = bazi.get_heavenly_branch_ymdh_pillars_base(biden_birthday.year,\n",
        "                                                            biden_birthday.month,\n",
        "                                                            biden_birthday.day,\n",
        "                                                            biden_birthday.hour)\n",
        "# 3. Calculate features\n",
        "\n",
        "ppl_start_date = today - timedelta(days=20)\n",
        "ppl_end_date = today + timedelta(days=120)\n",
        "ppl_time_range = pd.date_range(start=ppl_start_date, end=ppl_end_date, freq='1H').union(pd.date_range(end_date, end_date + pd.DateOffset(months=12), freq='D'))\n",
        "ppl_dataset = pd.DataFrame({'time': ppl_time_range})\n",
        "ppl_dataset = chengseng.adding_8w_pillars(ppl_dataset)\n",
        "\n",
        "# ppl_dataset['time'] = ppl_dataset['time'].dt.tz_localize('Asia/Hong_Kong')\n",
        "ppl_dataset['本時'] = trum_birthday_8w[\"時\"]\n",
        "ppl_dataset['本日'] = trum_birthday_8w[\"日\"]\n",
        "ppl_dataset['-本時'] = trum_birthday_8w[\"-時\"]\n",
        "ppl_dataset['本月'] = trum_birthday_8w[\"月\"]\n",
        "ppl_dataset['本年'] = trum_birthday_8w[\"年\"]\n",
        "ppl_dataset['-本月'] = trum_birthday_8w[\"-月\"]\n",
        "\n",
        "ppl_dataset.columns.to_list()\n",
        "ppl_dataset = chengseng.create_chengseng_for_dataset(ppl_dataset)\n",
        "ppl_dataset = add_haap_features_to_df(ppl_dataset)\n",
        "# 4. Encode features\n",
        "\n",
        "ppl_data_with_custom_features, encoder = process_encode_data(ppl_dataset, categorical_features)\n",
        "\n",
        "ppl_data_with_custom_features['Open'] = None\n",
        "ppl_data_with_custom_features['High'] = None\n",
        "ppl_data_with_custom_features['Low'] = None\n",
        "ppl_data_with_custom_features['Close'] = None\n",
        "\n",
        "ppl_future_features = ppl_data_with_custom_features.drop(['time', 'Open', 'High', \"Low\", \"Close\",\n",
        "                                                  # \"Volume\" ,\n",
        "                                                        # '流時',\n",
        "                                                        # '流日',\n",
        "                                                        # '-流時',\n",
        "                                                        # '流月',\n",
        "                                                        # '流年',\n",
        "                                                        # '-流月',\n",
        "                                                        '本時',\n",
        "                                                        '本日',\n",
        "                                                        '-本時',\n",
        "                                                        '本月',\n",
        "                                                        '本年',\n",
        "                                                        '-本月'], axis=1)\n",
        "\n",
        "ppl_future_predictions = mor_model.predict(ppl_future_features)\n",
        "ppl_data_with_custom_features[['Predicted_Open', 'Predicted_High', 'Predicted_Low', 'Predicted_Close']] = ppl_future_predictions\n",
        "\n",
        "ppl_df_plot = ppl_data_with_custom_features.copy()\n",
        "# Calculate the date range\n",
        "start_date = today - datetime.timedelta(days=25)\n",
        "end_date = today + datetime.timedelta(days=90)\n",
        "\n",
        "\n",
        "start_date = pd.to_datetime(start_date).tz_localize('Asia/Hong_Kong')\n",
        "end_date = pd.to_datetime(end_date).tz_localize('Asia/Hong_Kong')\n",
        "# Filter the DataFrame based on the date range\n",
        "df_plot = df_plot[(df_plot['time']>= start_date) & (df_plot['time'] <= end_date)]\n",
        "\n",
        "\n",
        "plot_actual_vs_predicted_vLine(ppl_df_plot, today, symbol_code, end_date)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2wmTX7QtUVK"
      },
      "outputs": [],
      "source": [
        "# @title Plot the graph\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.dates as mdates\n",
        "import mpl_tc_fonts\n",
        "import pytz\n",
        "import mplfinance as mpf\n",
        "\n",
        "mpl_tc_fonts.set_font(\"Noto Sans CJK TC\")\n",
        "\n",
        "# Assuming 'time' is a regular column (not the index)\n",
        "data_with_custom_features['time'] = pd.to_datetime(data_with_custom_features['time'])\n",
        "\n",
        "def save_plot(plt, today, symbol_code, end_date):\n",
        "    # Save the plot in PDF format\n",
        "    TARGET_COLUMN = \"Ken\" + today.strftime('%Y-%m-%d')\n",
        "    file_name_predict_with = symbol_code\n",
        "    plt.savefig(f\"drive/MyDrive/01_888/Prediction/prediction_hourly_{TARGET_COLUMN}_{end_date}_{file_name_predict_with}_version.pdf\", format=\"pdf\")\n",
        "\n",
        "\n",
        "def plot_actual_vs_predicted(data_with_custom_features, today, symbol_code, end_date):\n",
        "    # Assuming 'time' is the index of your DataFrame\n",
        "    plt.figure(figsize=(420, 60))\n",
        "    plt.plot(data_with_custom_features['time'], data_with_custom_features['Close'], label='Actual Close', marker='o')\n",
        "    plt.plot(data_with_custom_features['time'], data_with_custom_features['Predicted_Close'], label='Predicted Close', marker='o', linestyle='dashed')\n",
        "\n",
        "    plt.title('Actual vs Predicted Close Values')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Close Values')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    save_plot(today, symbol_code, end_date)\n",
        "    # # Save the plot in PDF format\n",
        "    # TARGET_COLUMN = \"Ken\" + today.strftime('%Y-%m-%d')\n",
        "    # file_name_predict_with = symbol_code\n",
        "    # plt.savefig(f\"drive/MyDrive/01_888/Prediction/prediction_hourly_{TARGET_COLUMN}_{end_date}_{file_name_predict_with}_version.pdf\", format=\"pdf\")\n",
        "\n",
        "DATE_TIME = 'time'\n",
        "\n",
        "def decode_and_annotate(row, feature, label_encoder, custom_encoding_mapping, ax, weight='normal', color='black', offset=55):\n",
        "    try:\n",
        "\n",
        "      decoded_value = next(key for key, value in custom_encoding_mapping.items() if value == label_encoder.inverse_transform([row[feature]]))\n",
        "      ax.annotate(f\"{feature}: {decoded_value}\", (row['time'], row['Predicted_Close']),\n",
        "                textcoords=\"offset points\", xytext=(5, offset), rotation=90, ha='center',\n",
        "                weight=weight, color=color)\n",
        "    except Exception as ex:\n",
        "      print(f\"error {feature} {ex}\")\n",
        "\n",
        "\n",
        "def plot_actual_vs_predicted_vLine(input_data, today, symbol_code, end_date):\n",
        "\n",
        "    data_with_custom_features = input_data.copy()\n",
        "\n",
        "    # Assuming 'time' is a column in your DataFrame with datetime values in Asia/Shanghai timezone\n",
        "    # data_with_custom_features['time'] = pd.to_datetime(data_with_custom_features['time']).dt.tz_localize('Asia/Shanghai')\n",
        "\n",
        "    # Assuming 'time' is a column in your DataFrame with datetime values in Asia/Shanghai timezone\n",
        "    data_with_custom_features['time'] = pd.to_datetime(data_with_custom_features['time'])\n",
        "\n",
        "    # Assuming 'time' is the index of your DataFrame\n",
        "    plt.figure(figsize=(380, 30))\n",
        "\n",
        "    # Plot the actual and predicted close values\n",
        "    plt.plot(data_with_custom_features['time'], data_with_custom_features['Close'], label='Actual Close', marker='o', color='blue')\n",
        "    plt.plot(data_with_custom_features['time'], data_with_custom_features['Predicted_Close'], label='Predicted Close', marker='+', linestyle='dashed', color='red')\n",
        "\n",
        "\n",
        "    # Set x-axis ticks and labels for every 2 hours with 'yyyy.mm.dd hh mm' format\n",
        "    date_format = mdates.DateFormatter('%Y-%m-%d %H:%M:%S%z')\n",
        "    plt.gca().xaxis.set_major_formatter(date_format)\n",
        "\n",
        "    plt.xticks(pd.date_range(start=data_with_custom_features['time'].min(),\n",
        "                            end=data_with_custom_features['time'].max(), freq='2H'), rotation=45, ha='right', minor=False)\n",
        "\n",
        "    # # Add vertical dotted lines for every 2 hours\n",
        "    # for time_point in pd.date_range(start=data_with_custom_features['time'].min(), end=data_with_custom_features['time'].max(), freq='2H'):\n",
        "    #     plt.axvline(x=time_point, color='gray', linestyle='--', linewidth=0.5)\n",
        "\n",
        "      # Increase the number of x-axis ticks\n",
        "    num_ticks = 150  # Choose the number of ticks you want\n",
        "    x_ticks_indices = range(0, len(data_with_custom_features[DATE_TIME]), len(data_with_custom_features[DATE_TIME]) // num_ticks)\n",
        "    x_ticks_labels = data_with_custom_features[DATE_TIME].iloc[x_ticks_indices]\n",
        "\n",
        "    # Add actual values next to each dot\n",
        "    for index, row in data_with_custom_features.iterrows():\n",
        "        plt.annotate(f\"{row['Close']:.2f}\", (row['time'], row['Close']),\n",
        "                     textcoords=\"offset points\", xytext=(5,-55), rotation=45, ha='center')\n",
        "        plt.annotate(f\"{row['Predicted_Close']:.2f}\", (row['time'], row['Predicted_Close']),\n",
        "                     textcoords=\"offset points\", xytext=(5,15),rotation=45, weight='bold', ha='center')\n",
        "        # decoded_value_流月 = label_encoder.inverse_transform([row['流月']])\n",
        "        # decoded_value_流月 = next(key for key, value in custom_encoding_mapping.items() if value == row['流月'])\n",
        "        # decoded_value_流月 = next(key for key, value in custom_encoding_mapping.items() if value == label_encoders[0].inverse_transform([row['流月']]))\n",
        "        # plt.annotate(f\"月:{decoded_value_流月}\", (row['time'], row['Predicted_Close']),\n",
        "        #              textcoords=\"offset points\", xytext=(5,55),rotation=90, ha='center')\n",
        "        # Assuming '流月' is the feature you want to decode and annotate\n",
        "        decode_and_annotate(row, '-流月', label_encoders['-流月'], custom_encoding_mapping, offset = 305, ax=plt.gca())\n",
        "        decode_and_annotate(row, '流年', label_encoders['流年'], custom_encoding_mapping, offset = 255, ax=plt.gca())\n",
        "        decode_and_annotate(row, '流月', label_encoders['流月'], custom_encoding_mapping, offset = 205, ax=plt.gca(), weight='bold', color='red')\n",
        "        decode_and_annotate(row, '-流時', label_encoders['-流時'], custom_encoding_mapping, offset = 155, ax=plt.gca())\n",
        "        decode_and_annotate(row, '流日', label_encoders['流日'], custom_encoding_mapping, offset = 105, ax=plt.gca(),  color='blue')\n",
        "        decode_and_annotate(row, '流時', label_encoders['流時'], custom_encoding_mapping, offset = 55, ax=plt.gca(),  color = 'blue')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# '流時',\n",
        "# '流日',\n",
        "# '-流時',\n",
        "# '流月',\n",
        "# '流年',\n",
        "# '-流月',\n",
        "    # Set labels and title\n",
        "    plt.title('Actual vs Predicted Close Values')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Close Values')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Adjust y-axis limits to increase spacing\n",
        "    plt.ylim(bottom=min(data_with_custom_features[['Close', 'Predicted_Close']].min()) - 1000,\n",
        "             top=max(data_with_custom_features[['Close', 'Predicted_Close']].max()) + 1200)\n",
        "\n",
        "    # Adjust spacing around the plot\n",
        "    #plt.subplots_adjust(top=1.1, bottom=1)\n",
        "\n",
        "    # Save the plot in PDF format\n",
        "    TARGET_COLUMN = \"Ken\" + today.strftime('%Y-%m-%d%hh%mm%ss')\n",
        "    file_name_predict_with = symbol_code\n",
        "    #mbol_code\n",
        "    plt.plot()\n",
        "\n",
        "    plt.savefig(f\"drive/MyDrive/01_888/Prediction/prediction_hourly_{TARGET_COLUMN}_{end_date}_{file_name_predict_with}_version.pdf\", format=\"pdf\")\n",
        "\n",
        "    plt.show()\n",
        "    # Show the plot\n",
        "    plt.close()\n",
        "    #save_plot(plt.gca(), today, symbol_code, end_date)\n",
        "\n",
        "def plot_actual_vs_predicted_candle(input_data, today, symbol_code, end_date):\n",
        "    \"\"\"\n",
        "    Plot actual vs predicted candlestick patterns for a given date range.\n",
        "\n",
        "    Parameters:\n",
        "        input_data (DataFrame): Input data containing historical price data.\n",
        "        today (str): Date for which you want to compare actual and predicted candlestick patterns (format: 'YYYY-MM-DD').\n",
        "        symbol_code (str): Symbol code for the asset.\n",
        "        end_date (str): End date of the date range (format: 'YYYY-MM-DD').\n",
        "    \"\"\"\n",
        "    # Filter data for the specified date range\n",
        "    filtered_data = input_data.copy()\n",
        "    # Assuming 'time' is the index of your DataFrame\n",
        "    # Set the size of the figure\n",
        "    plt.figure(figsize=(1200, 6))\n",
        "\n",
        "\n",
        "    # Extract actual and predicted data for the specified date\n",
        "    actual_data = filtered_data['Close']\n",
        "    predicted_data = filtered_data[['Predicted_Open', 'Predicted_High', 'Predicted_Low', 'Predicted_Close']]  # Replace this with your predicted data\n",
        "    predicted_data.columns = ['Open', 'High', 'Low', 'Close']  # Rename columns for compatibility with mplfinance\n",
        "    predicted_data.index = filtered_data['time']\n",
        "    # Plot actual candlestick chart\n",
        "    fig, ax = plt.subplots(figsize=(150, 20))\n",
        "    # plt.plot(filtered_data['time'], filtered_data['Close'], label='Actual Close', marker='o', color='blue')\n",
        "    # Increase the number of x-axis ticks\n",
        "    # num_ticks = 150  # Choose the number of ticks you want\n",
        "    # x_ticks_indices = range(0, len(data_with_custom_features[DATE_TIME]), len(data_with_custom_features[DATE_TIME]) // num_ticks)\n",
        "    # x_ticks_labels = data_with_custom_features[DATE_TIME].iloc[x_ticks_indices]\n",
        "\n",
        "    # plt.xticks(pd.date_range(start=filtered_data['time'].min(),\n",
        "    #                         end=filtered_data['time'].max(), freq='2H'), rotation=45, ha='right', minor=False)\n",
        "    # Overlay predicted candlestick chart\n",
        "    predicted_plot = mpf.plot(predicted_data, type='candle', style='starsandstripes', ax=ax)\n",
        "\n",
        "    # # Check if plots were successfully created before adding title\n",
        "    # if predicted_plot is not None:\n",
        "    #     plt.suptitle(f'Actual vs Predicted Candlestick Chart - {symbol_code}')\n",
        "    #     plt.show()\n",
        "    # else:\n",
        "    #     print(\"Error: Failed to create plots.\")\n",
        "\n",
        "    # Set x-axis ticks every two hours\n",
        "    # tick_locations = pd.date_range(start=filtered_data['time'].min(), end=filtered_data['time'].max(), freq='2H')\n",
        "    # tick_labels = [tick.strftime('%Y-%m-%d %H:%M:%S') for tick in tick_locations]\n",
        "    # plt.xticks(tick_locations, tick_labels)\n",
        "\n",
        "    # Add title\n",
        "    plt.title(f'Actual vs Predicted Candlestick Chart - {symbol_code}')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "df_plot = data_with_custom_features.copy()\n",
        "# Calculate the date range\n",
        "start_date = today - datetime.timedelta(days=25)\n",
        "end_date = today + datetime.timedelta(days=90)\n",
        "\n",
        "\n",
        "start_date = pd.to_datetime(start_date).tz_localize('Asia/Hong_Kong')\n",
        "end_date = pd.to_datetime(end_date).tz_localize('Asia/Hong_Kong')\n",
        "# Filter the DataFrame based on the date range\n",
        "df_plot = df_plot[(df_plot['time']>= start_date) & (df_plot['time'] <= end_date)]\n",
        "\n",
        "\n",
        "# plot_actual_vs_predicted_vLine(df_plot, today, symbol_code, end_date)\n",
        "plot_actual_vs_predicted_candle(df_plot, today, symbol_code, end_date)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "aGC-rUmtA9PK"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
